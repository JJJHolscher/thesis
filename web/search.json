[
  {
    "objectID": "elbo.html",
    "href": "elbo.html",
    "title": "ELBO",
    "section": "",
    "text": "This project works with diffusion models, which seem to sort-of do the same thing as variational auto-encoders do. They model some kind of data distribution.\nFor some reason, VAEs cannot directly optimize themselves to maximize the P(distribution). (What is P(distribution)?) but optimize for some lower bound.\nWhy? This ELBO keeps coming up and I will need to write about the theoretical foundations of my thesis at some point, so let’s just dig into it.\n\n\nI’m likely going to use OpenAI’s ImageNet models, so I was reading the paper. Not that I need to for using them (probably), but if I’m going to use their models, I should cite their paper. It seems appropriate to read it then, and it might make it easier for me to then alter parts of the model since I will need to do that.\nHere it is. https://arxiv.org/abs/2102.09672\nNow, here they reference the Variational Lower Bound, which google tells me is another way of saying “Evidence Lower Bound” (ELBo).\n\n\n\nPaul Rubenstein’s blog post seems to derive the ELBO nicely for us. But I got distracted by the very common practice of optimizing for the log of the function instead of the function itself. I trust it’s legal to do, but lets go beyond trust this time.\nI wanted to find out whether the derivative of log(x) is proportional to that of x. I forgot what the derivative of that was, so I derived it myself.\n\nI gave up and peeked. There I was surprised:\n\\(\\lim_{t → 0}(1 + t)^{1/t} = e\\)?\n\n\nSome video (don’t watch it yet). Immediately mentions the Binomial Theorem.\nApparently, someone found a general formula for finding the coefficients of \\((a + b)^x\\) after it is expanded. Here expanding means, you remove the parenthesis and write the entire thing out. Wikipedia’s summary was very helpful here.\nApparently \\(\\binom{n}{k}\\) means, “give the \\(k\\)th coefficient of the expanded form of \\((a + b)^n\\)”. Go read the wikipedia article for more.\n\n\n\nThis site points me to the question I should have asked myself.\nThe one thing I know about \\(e\\) is that \\(\\frac{d}{dx} e^x = e^x\\).\nSo, if we get back to limits, we can find \\(e\\) by solving \\(a\\) for\n\\(\\frac{d}{dx} a^x = \\lim_{h → 0} \\frac{a^{x+h} - a^x}{h} = a^x\\)\nThis turned out to be fairly easy:\n\nSo, yes \\(\\lim_{t → 0}(1 + t)^{1/t} = e\\)\nHmmm, it turns out I hadn’t needed Google. That part with the Binomial Theorem primarily was completely unnecessary.\n\n\n\n\\[\\frac{d}{dx} \\ln(f(x)) = \\frac{1}{f(x)} · \\frac{d}{dx} f(x)\\]\nThis is a factor of \\(\\frac{1}{f(x)}\\) away from \\(\\frac{d}{dx} f(x)\\), which seems fine!\nJust remember to multiply by \\(f(x)\\) whenever you use \\(\\frac{d}{dx} \\ln f(x)\\) as a substitute for \\(\\frac{d}{dx} f(x)\\).\n\n\n\n\nSo, back to the blog post.\nWe want to find \\(p(x)\\), the distribution that models all points in a dataset \\(x \\in X\\) and for our generative purposes, we assume that there exists some encoding \\(z\\) that gives our generator information on the \\(x\\) that ought be generated.\n\n\nI’m uncertain, but I think \\(z\\) is necessary due to the discreteness of our computation.\nWe want to model \\(p(x)\\), not just any x. \\(p(x)\\) is a distribution with a lot of information, you don’t get a model to output that entire distribution, you only get a model that can sample from that distribution.\nSo, we are building a model that (approximately) samples \\(p(x)\\). This way, we need some index that informs our model which \\(x\\) we want from \\(p(x)\\).\nAnd what would be extra nice, is if that index already encodes some information about the likelihood of that individual \\(x\\).\nI mean to say, that the instruction you give to the generative model is of the form:\n\nplease generate me an x, that is rare with regards to property \\(z_1, z_2\\) and \\(z_3\\), but has a very typical \\(z_4\\).\n\nSo before it starts generating, we already know how typical the generated \\(x\\) will be, or how likely we are to find points in the dataset that are similar to \\(x\\).\nThis is done by having \\(z\\) be from a known distribution, always usually by sampling \\(z \\sim \\mathcal{N}(0; \\textbf{I})\\). So, you know when the absolute \\(z_i\\) is large, the generated \\(x\\) will be unlikely with respect to the ith element.\nFinally, let it be known that \\(z\\) is not some interpretable variable. \\(z\\) contains information about \\(x\\), but only the model knows how to use that.\n\n\n\nSo, with our z, we get:\n\\(p(x) = \\int p(x|z) p(z|x) dz\\)\nWhere x is the data we want to model, and z is some encoding of x.\nNow, if we have some model with parameters θ that can generate \\(x\\)s from \\(z\\)s (\\(p_θ(x|z)\\)) then you’d think we could cleverly invert that model somehow such that we can also convert \\(z\\)’s to \\(x\\)’s (\\(p_θ(z|x)\\)).\nWell, that’s theoretically true, but neural networks are too complex for that inversion to be feasible, so we need a separate, additional model called “the encoder” that approximates \\(p(z|x)\\), we’ll call it \\(q_φ(z|x)\\).\n\n\n\nNow, we don’t actually do away with \\(p(z|x)\\) just yet, but using some foresight, we rewrite \\(p(x)\\) as:\n\\[\np(x) = \\int p(x|z) p(z|x) \\frac{q(z|x)}{q(z|x)}dz  \n\\] \\[\n\\ln p(x) = \\ln \\int p(x|z) p(z|x) \\frac{q(z|x)}{q(z|x)}dz\n\\]\nThen… you should just read Paul Rubenstein’s blog post it’s shorter than this. He goes over the rest of the math to get to ELBO, but that looks understandable to me.\nI could re-write them here, but then I’d be pretending to have done mental work I haven’t. I’d just by copy-pasting.\nIf you don’t want to go there, then the main point is that you can separate \\(p(z|x)\\) out of the rest of the above formula into a part that guaranteed to be positive or 0.\nYou ignore that part, and optimize for the rest, which is the ELBO.\nThe main thing I still want to understand is Jensen’s Inequality, which I might add here later."
  },
  {
    "objectID": "elbo.html#how-we-got-here.",
    "href": "elbo.html#how-we-got-here.",
    "title": "ELBO",
    "section": "",
    "text": "I’m likely going to use OpenAI’s ImageNet models, so I was reading the paper. Not that I need to for using them (probably), but if I’m going to use their models, I should cite their paper. It seems appropriate to read it then, and it might make it easier for me to then alter parts of the model since I will need to do that.\nHere it is. https://arxiv.org/abs/2102.09672\nNow, here they reference the Variational Lower Bound, which google tells me is another way of saying “Evidence Lower Bound” (ELBo)."
  },
  {
    "objectID": "elbo.html#sidestepping-to-calculus-101",
    "href": "elbo.html#sidestepping-to-calculus-101",
    "title": "ELBO",
    "section": "",
    "text": "Paul Rubenstein’s blog post seems to derive the ELBO nicely for us. But I got distracted by the very common practice of optimizing for the log of the function instead of the function itself. I trust it’s legal to do, but lets go beyond trust this time.\nI wanted to find out whether the derivative of log(x) is proportional to that of x. I forgot what the derivative of that was, so I derived it myself.\n\nI gave up and peeked. There I was surprised:\n\\(\\lim_{t → 0}(1 + t)^{1/t} = e\\)?\n\n\nSome video (don’t watch it yet). Immediately mentions the Binomial Theorem.\nApparently, someone found a general formula for finding the coefficients of \\((a + b)^x\\) after it is expanded. Here expanding means, you remove the parenthesis and write the entire thing out. Wikipedia’s summary was very helpful here.\nApparently \\(\\binom{n}{k}\\) means, “give the \\(k\\)th coefficient of the expanded form of \\((a + b)^n\\)”. Go read the wikipedia article for more.\n\n\n\nThis site points me to the question I should have asked myself.\nThe one thing I know about \\(e\\) is that \\(\\frac{d}{dx} e^x = e^x\\).\nSo, if we get back to limits, we can find \\(e\\) by solving \\(a\\) for\n\\(\\frac{d}{dx} a^x = \\lim_{h → 0} \\frac{a^{x+h} - a^x}{h} = a^x\\)\nThis turned out to be fairly easy:\n\nSo, yes \\(\\lim_{t → 0}(1 + t)^{1/t} = e\\)\nHmmm, it turns out I hadn’t needed Google. That part with the Binomial Theorem primarily was completely unnecessary.\n\n\n\n\\[\\frac{d}{dx} \\ln(f(x)) = \\frac{1}{f(x)} · \\frac{d}{dx} f(x)\\]\nThis is a factor of \\(\\frac{1}{f(x)}\\) away from \\(\\frac{d}{dx} f(x)\\), which seems fine!\nJust remember to multiply by \\(f(x)\\) whenever you use \\(\\frac{d}{dx} \\ln f(x)\\) as a substitute for \\(\\frac{d}{dx} f(x)\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "ELBO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethodology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech Stack Q&A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "",
    "section": "",
    "text": "graph LR\n    img([1,28,28]) --- conv\n    conv --- a1\n    a1([3,25,25]) --- max-pool\n    max-pool -- _/ --- a2\n    a2([3,24,24]) --- ravel\n    ravel --- a3\n    a3([1728]) --- a( )\n\n\n\n\n\n\n\n\n\n\n\ngraph LR\n    a( ) --- linear-1\n    linear-1 -- σ --- a4\n    a4([512]) --- linear-2\n    linear-2 -- _/ --- a5\n    a5([64]) --- linear-3\n    linear-3 -- eˣ/Σeˣ --- a6([10])\n\n\n\n\n\n\n\n\n\n\nread the TinyImageNet paper\nin what ways do models perform worse on TinyImageNet?\n\n\n\nTinyImageNet is a scaled-down version of the much larger ImageNet dataset, both of which are pivotal in the field of computer vision for training machine learning models, particularly in image classification tasks. Here’s a comparison based on their characteristics and use cases:\nSize and Complexity:\nImageNet: It’s one of the largest datasets available for image classification, containing over 14 million hand-annotated images divided into approximately 22,000 categories. The images in ImageNet are of various sizes and high resolution. TinyImageNet: It significantly reduces the scale, containing 100,000 images across 200 classes, with each image resized to a small dimension of 64x64 pixels. Computational Requirements:\nImageNet requires considerable computational resources for training due to its size and the complexity of its images. Training models on ImageNet often requires GPUs or even clusters of GPUs to complete in a reasonable amount of time. TinyImageNet, with its reduced size and complexity, requires significantly less computational power, making it more accessible for researchers with limited resources, for educational purposes, and for rapid prototyping. Use Cases:\nImageNet is used in cutting-edge research for developing state-of-the-art machine learning models in image recognition, capable of understanding intricate and nuanced distinctions between thousands of categories. TinyImageNet is often used for educational purposes, allowing students and researchers to experiment with neural network architectures without the need for extensive computational resources. It serves as a testbed for algorithm development and for those new to the field of machine learning. Performance Benchmarking:\nImageNet is the benchmark for many competitions and research works in computer vision, where the accuracy of models on this dataset is a key indicator of their performance and sophistication. TinyImageNet, while not as widely used as a benchmark in high-profile competitions, offers a simpler benchmarking tool for evaluating the performance of lightweight or less complex models. Accessibility:\nImageNet’s large size makes it more challenging to download, store, and process. TinyImageNet, being much smaller, is easier to download and work with, especially for individuals and organizations with limited storage and computing power. In summary, while ImageNet remains the gold standard for training and benchmarking advanced image classification models, TinyImageNet provides a practical alternative for situations requiring lower computational costs, educational purposes, and initial experiments in model development. The choice between the two largely depends on the specific needs, resources, and goals of the project or learning endeavor.\nModels trained on TinyImageNet, due to the dataset’s reduced scale and complexity compared to ImageNet, may exhibit certain limitations in capabilities:\nGeneralization to High-Resolution Images: TinyImageNet images are of low resolution (64x64 pixels), which means models trained on this dataset might struggle with high-resolution images found in real-world applications. ImageNet models, trained on a diverse set of high-resolution images, are better at handling and recognizing details in such images.\nDetailed Feature Recognition: The lower resolution and simplified dataset mean that models trained on TinyImageNet might not learn to recognize as detailed features as those trained on ImageNet. The intricate textures, patterns, and object details present in ImageNet’s high-resolution images facilitate deeper feature extraction capabilities in models.\nCategory Breadth and Depth: ImageNet encompasses over 22,000 categories, offering a far more extensive range of objects, scenes, and concepts for models to learn from, compared to TinyImageNet’s 200 classes. This broad exposure helps ImageNet-trained models to classify a wider variety of images accurately and with more nuanced understanding.\nRobustness and Adaptability: Training on a larger and more complex dataset like ImageNet exposes models to a wider range of lighting conditions, angles, backgrounds, and occlusions, making them more robust and adaptable to different real-world conditions. TinyImageNet-trained models might not exhibit the same level of robustness due to the dataset’s limitations.\nFine-Grained Classification: The extensive number of categories in ImageNet, including many subcategories for animals, objects, and scenes, allows for fine-grained classification. Models trained on ImageNet can distinguish between closely related categories (e.g., different dog breeds). TinyImageNet’s limited category set provides less opportunity for learning such fine distinctions.\nTransfer Learning and Feature Extraction: While models trained on both datasets can be used for transfer learning, ImageNet models are often more effective as feature extractors for other tasks beyond image classification. The depth and breadth of learned features make ImageNet-trained models more versatile when applied to other domains or tasks.\nReal-World Application Performance: Given the aforementioned points, models trained on ImageNet are generally more prepared for direct application to real-world problems, which often involve complex scenes and high-resolution images. Models trained on TinyImageNet may require additional fine-tuning or may only be suitable for applications where the image conditions closely match those of the TinyImageNet dataset.\nIn summary, while TinyImageNet offers a valuable resource for education, experimentation, and research where computational resources are limited, the capabilities of models trained on this dataset are constrained compared to those trained on ImageNet. These constraints primarily involve generalization, detail recognition, category coverage, and real-world application readiness.",
    "crumbs": [
      "Experiments"
    ]
  },
  {
    "objectID": "experiments.html#experiments",
    "href": "experiments.html#experiments",
    "title": "",
    "section": "",
    "text": "graph LR\n    img([1,28,28]) --- conv\n    conv --- a1\n    a1([3,25,25]) --- max-pool\n    max-pool -- _/ --- a2\n    a2([3,24,24]) --- ravel\n    ravel --- a3\n    a3([1728]) --- a( )\n\n\n\n\n\n\n\n\n\n\n\ngraph LR\n    a( ) --- linear-1\n    linear-1 -- σ --- a4\n    a4([512]) --- linear-2\n    linear-2 -- _/ --- a5\n    a5([64]) --- linear-3\n    linear-3 -- eˣ/Σeˣ --- a6([10])\n\n\n\n\n\n\n\n\n\n\nread the TinyImageNet paper\nin what ways do models perform worse on TinyImageNet?\n\n\n\nTinyImageNet is a scaled-down version of the much larger ImageNet dataset, both of which are pivotal in the field of computer vision for training machine learning models, particularly in image classification tasks. Here’s a comparison based on their characteristics and use cases:\nSize and Complexity:\nImageNet: It’s one of the largest datasets available for image classification, containing over 14 million hand-annotated images divided into approximately 22,000 categories. The images in ImageNet are of various sizes and high resolution. TinyImageNet: It significantly reduces the scale, containing 100,000 images across 200 classes, with each image resized to a small dimension of 64x64 pixels. Computational Requirements:\nImageNet requires considerable computational resources for training due to its size and the complexity of its images. Training models on ImageNet often requires GPUs or even clusters of GPUs to complete in a reasonable amount of time. TinyImageNet, with its reduced size and complexity, requires significantly less computational power, making it more accessible for researchers with limited resources, for educational purposes, and for rapid prototyping. Use Cases:\nImageNet is used in cutting-edge research for developing state-of-the-art machine learning models in image recognition, capable of understanding intricate and nuanced distinctions between thousands of categories. TinyImageNet is often used for educational purposes, allowing students and researchers to experiment with neural network architectures without the need for extensive computational resources. It serves as a testbed for algorithm development and for those new to the field of machine learning. Performance Benchmarking:\nImageNet is the benchmark for many competitions and research works in computer vision, where the accuracy of models on this dataset is a key indicator of their performance and sophistication. TinyImageNet, while not as widely used as a benchmark in high-profile competitions, offers a simpler benchmarking tool for evaluating the performance of lightweight or less complex models. Accessibility:\nImageNet’s large size makes it more challenging to download, store, and process. TinyImageNet, being much smaller, is easier to download and work with, especially for individuals and organizations with limited storage and computing power. In summary, while ImageNet remains the gold standard for training and benchmarking advanced image classification models, TinyImageNet provides a practical alternative for situations requiring lower computational costs, educational purposes, and initial experiments in model development. The choice between the two largely depends on the specific needs, resources, and goals of the project or learning endeavor.\nModels trained on TinyImageNet, due to the dataset’s reduced scale and complexity compared to ImageNet, may exhibit certain limitations in capabilities:\nGeneralization to High-Resolution Images: TinyImageNet images are of low resolution (64x64 pixels), which means models trained on this dataset might struggle with high-resolution images found in real-world applications. ImageNet models, trained on a diverse set of high-resolution images, are better at handling and recognizing details in such images.\nDetailed Feature Recognition: The lower resolution and simplified dataset mean that models trained on TinyImageNet might not learn to recognize as detailed features as those trained on ImageNet. The intricate textures, patterns, and object details present in ImageNet’s high-resolution images facilitate deeper feature extraction capabilities in models.\nCategory Breadth and Depth: ImageNet encompasses over 22,000 categories, offering a far more extensive range of objects, scenes, and concepts for models to learn from, compared to TinyImageNet’s 200 classes. This broad exposure helps ImageNet-trained models to classify a wider variety of images accurately and with more nuanced understanding.\nRobustness and Adaptability: Training on a larger and more complex dataset like ImageNet exposes models to a wider range of lighting conditions, angles, backgrounds, and occlusions, making them more robust and adaptable to different real-world conditions. TinyImageNet-trained models might not exhibit the same level of robustness due to the dataset’s limitations.\nFine-Grained Classification: The extensive number of categories in ImageNet, including many subcategories for animals, objects, and scenes, allows for fine-grained classification. Models trained on ImageNet can distinguish between closely related categories (e.g., different dog breeds). TinyImageNet’s limited category set provides less opportunity for learning such fine distinctions.\nTransfer Learning and Feature Extraction: While models trained on both datasets can be used for transfer learning, ImageNet models are often more effective as feature extractors for other tasks beyond image classification. The depth and breadth of learned features make ImageNet-trained models more versatile when applied to other domains or tasks.\nReal-World Application Performance: Given the aforementioned points, models trained on ImageNet are generally more prepared for direct application to real-world problems, which often involve complex scenes and high-resolution images. Models trained on TinyImageNet may require additional fine-tuning or may only be suitable for applications where the image conditions closely match those of the TinyImageNet dataset.\nIn summary, while TinyImageNet offers a valuable resource for education, experimentation, and research where computational resources are limited, the capabilities of models trained on this dataset are constrained compared to those trained on ImageNet. These constraints primarily involve generalization, detail recognition, category coverage, and real-world application readiness.",
    "crumbs": [
      "Experiments"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "Results"
  },
  {
    "objectID": "tech-stack.html",
    "href": "tech-stack.html",
    "title": "Tech Stack Q&A",
    "section": "",
    "text": "Tech Stack Q&A\n\n\n\n\n\nwhat am I looking at?\n\nIt’s called “feature-conditional generation” or something like it, the github is called “features2image_diffusion”. It comes down to using a traditional conditional diffusion model with some other network’s activations instead of text embeddings of input text.\n\nfor what purpose?\n\nInterpreting a network’s activations. The diffusion model generates data points that look like the original dataset. If you condition it on the activations, you can see what kind of data the model generates if some activation is exaggerated.\n\nuhh\n\nSee the diagram next to this text.\n\n…\n\nSo, we have a CNN on the right which is our target for understanding.\n\n“target for understandig”?\n\nThat which we want to understand.\n\ngo on\n\nThere are various facets we can try to comprehend about the CNN. I chose to further understanding about the activations of the model.\n\nwhy the activations?\n\nThere isn’t really a correct answer to this. At the end, we need to understand everything: the weights, the architecture, the role of the nonlinearities and the semantics of the activations. I chose the activations since, intuitively, those seem to contain information we might recognise better.\n\nWhy would we recognise information in activations better than in other parts like the weights?\n\nThe activations depend on the specific data point that we feed into the CNN. This specific data point is recognisable to us, so that is a nice anchor point on which we can build further understandings.\nWeights on the other hand, are constructed during the entire training run and therefore depend on every data point in the set. For us to understand those, we need something different to anchor on, one might anchor on algorithms we would expect networks to implement for example. Which anchor to pick there is far less clear however and since nobody seems to have yet succeeded in making broad and accurate guesses about what kind of algorithms networks implement, we currently don’t know what kind of anchors are useful for building our understanding upon there.\n\nWith “anchor”, do you mean some concrete thing we can reference back to when explaining some part of the CNN?\n\nYes, I explain the activations of the CNN by showing permutations of the original data point that caused those activations. So the data point is the anchor.\nI also explain the activations in terms of the prediction the CNN makes, that’s a second anchor.\n\nAlright, so you were explaining the diagram that showcases what you did.\n\nSo, on the right we have the CNN that we want to understand. In the diagram, I often also show the activations that flow between layers, any box in dotted lines are activations, any undotted box are layers. Lines between those signify activation flow, sometimes with a half-arrow to show the direction.\nYou can see that I copy the activations before the last layer of the CNN (since two lines flow out of it) and one of the copies goes into the sparse autoencoder (SAE).\n\nWhat is that SAE doing there?\n\nAnthropic claims that the elements of activations can be made less polysemantic when you encode them with a SAE.\n\nWhat is polysemanticity and why would we care?\n\nPolysemanticity is the property of an activation to encode multiple meanings in the same place.\nNeural networks need to deal with a lot of features, that is, properties of data points that are relevant to keep track of, for the network’s task.\nBut, only a small subset of all features in the data set, are present in any single data point. Having a neuron per feature would be inefficient and the network is better off with having the same neurons do different things depending on the context.\nA famous example is the fact that one later layer in the GoogLeNet vision model has a neuron that fires on cat faces and car doors. So that is 2 features in a single neuron. The likely explanation is that the rest of the network has little trouble differentiating cats from cars, so this neuron is allowed to be confused between the two.\nTo elaborate on this, when the rest of the network is uncertain about whether the image is a cat or a dog, this neuron helps by firing when there is a cat face. When the network is uncertain about whether there is a house or a car, it fires on car doors.\nThe instance where the network is uncertain about whether the image is a cat or a car is so rare (since earlier neurons successfully differentiate the two), that this particular neuron does not have to know the difference. It would only be wasteful to have a separate neuron per feature in that layer.\n\nOkay, so you convert the activation into its features.\n\nYes, I should mention that this is an optional step though.\n\nHow so?\n\nWell, the success of the SAE is not thoroughly tested. Plausibly it decomposes activations into their features, but there is no guarantee that those features are then more interpretable.\nWe can also keep the activations as-is. I’ll keep calling these copied activations that may or may not have been encoded “features” though.\n\nOkay, so what are you then doing with these (maybe-encoded) features? You seem to feed them into the “UNET” at some halfway point.\n\nYes, the U-NET is the network used for diffusion, where diffusion is that process famous for generating images.\nWhen training a diffusion model, we sample an image from a data set, add noise to it and feed it into the U-NET. The task of the U-NET then, is to predict what part of its input is noise and what part is image.\nWe call it “noise prediction” since the U-NET needs to output the noise.\n\nHow does noise prediction lead to image generation?\n\nNow, what we can do with a trained U-NET, is to pass complete noise into it, and it then predicts which part of that was noise. It will not output the exact same as the input, since during its training it is used to receiving input that is part image, part noise. So it outputs noise that is similar, but not completely identical to the input.\nWe subtract this noise then from the input and feed that into the U-NET again. This process repeats for 400 iterations.\n\nSo, at the 400th iteration, you have subtracted all the noise?\n\nExactly, and what remains is an image that looks like an image from the dataset.\n\nOkay, so now you’ve generated an image from say, the MNIST data set. Where do your features come into play in this?\n\nWhat we have talked about thus far is normal diffusion: generate images from a dataset.\nNormal diffusion lacks direction into which kind of images from the dataset it generates. Maybe it generates a 1, maybe a 9, we have no control.\n\nSo you are going to use the features to steer the diffusion’s generation?\n\nYes, this is called conditional diffusion. This is done by inserting auxiliary information about the image you want to generate, halfway in the U-NET.\nThe most-used applications of diffusion (Stable Diffusion, DALL-E, Midjourney) do conditional diffusion with text. Note that you don’t just generate an image with those applications, you give it some description that then steers the generation.\nDuring training, they take an image and its caption, they tokenize and embed the caption into some array and then multiply this array with the activations of the U-NET at the midway point in the U-NET, while the U-NET is looking at some noisy version of the image.\nThe U-NET uses this new information, since this information tells the U-NET what kind of image it should generate: not just any image from the training set, but the specific image that would have this caption.\n\nOkay, so instead of feeding it text embeddings, you feed it the CNN’s features?\n\nYes. The image that the CNN got, is copied and made noisy, so the U-NET is looking at a noisy version of the same image. Then, at the midpoint, the U-NET receives the activations from the CNN, so it can further steer its generation based on the information the CNN’s features provide.\n\nOoo-kay, so now you have diffusion model that can generate images based on the CNN’s features.\n\nWe feed the CNN some image and intercept its features. Then we have the diffusion model generate an image while conditioning it on those features. The diffusion model then generates an image that is very close to the image that the CNN got. So, this part works.\n\nHow does this help with interpreting the CNN?\n\nWe have now a U-NET that is trained to interpret the CNN’s features.\n\nOkay, so what?\n\nSo what we can do now, is edit those features and see what kind of image the diffusion model generates.\nThen, by doing various edits on various parts of the features, we might learn what purposes individual features suit.\n\nHave you got any results?\n\nYes, though it is a work in progress. Right now I’m still training various U-NETs on the features of different architectures and sampling from them. Soon, I’ll upload more of my results.\n\n\n\n\n\n\nFeature-Conditional Diffusion\n\n\n\n\n\n\n\nFeature-Conditional Diffusion"
  },
  {
    "objectID": "elbo.html#the-binomial-theorem",
    "href": "elbo.html#the-binomial-theorem",
    "title": "ELBO",
    "section": "",
    "text": "Some video (don’t watch it yet). Immediately mentions the Binomial Theorem.\nApparently, someone found a general formula for finding the coefficients of \\((a + b)^x\\) after it is expanded. Here expanding means, you remove the parenthesis and write the entire thing out. Wikipedia’s summary was very helpful here.\nApparently \\(\\binom{n}{k}\\) means, “give the \\(k\\)th coefficient of the expanded form of \\((a + b)^n\\)”. Go read the wikipedia article for more."
  },
  {
    "objectID": "elbo.html#deriving-eulers-number",
    "href": "elbo.html#deriving-eulers-number",
    "title": "ELBO",
    "section": "",
    "text": "This site https://artofproblemsolving.com/wiki/index.php/Euler%27s_number points me to the question I should have asked myself.\nThe one thing I know about \\(e\\) is that \\(\\frac{d}{dx} e^x = e^x\\).\nSo, if we get back to limits, we can find \\(e\\) by solving \\(a\\) for\n\\(\\frac{d}{dx} a^x = \\lim_{h → 0} \\frac{a^{x+h} - a^x}{h} = a^x\\)\nThis turned out to be fairly easy:"
  },
  {
    "objectID": "elbo.html#how-we-got-here",
    "href": "elbo.html#how-we-got-here",
    "title": "ELBO",
    "section": "",
    "text": "I’m likely going to use OpenAI’s ImageNet models, so I was reading the paper. Not that I need to for using them (probably), but if I’m going to use their models, I should cite their paper. It seems appropriate to read it then, and it might make it easier for me to then alter parts of the model since I will need to do that.\nHere it is. https://arxiv.org/abs/2102.09672\nNow, here they reference the Variational Lower Bound, which google tells me is another way of saying “Evidence Lower Bound” (ELBo)."
  },
  {
    "objectID": "elbo.html#elbo-real",
    "href": "elbo.html#elbo-real",
    "title": "ELBO",
    "section": "",
    "text": "So, back to the blog post.\nWe want to find \\(p(x)\\), the distribution that models all points in a dataset \\(x \\in X\\) and for our generative purposes, we assume that there exists some encoding \\(z\\) that gives our generator information on the \\(x\\) that ought be generated.\n\n\nI’m uncertain, but I think \\(z\\) is necessary due to the discreteness of our computation.\nWe want to model \\(p(x)\\), not just any x. \\(p(x)\\) is a distribution with a lot of information, you don’t get a model to output that entire distribution, you only get a model that can sample from that distribution.\nSo, we are building a model that (approximately) samples \\(p(x)\\). This way, we need some index that informs our model which \\(x\\) we want from \\(p(x)\\).\nAnd what would be extra nice, is if that index already encodes some information about the likelihood of that individual \\(x\\).\nI mean to say, that the instruction you give to the generative model is of the form “please generate me an x, that very rare in property \\(z_1, z_2\\) and \\(z_3\\), but has a very typical \\(z_4\\)”.\nSo before it starts generating, we already know how typical the generated \\(x\\) will be, or how likely we are to find points in the dataset that are similar to \\(x\\).\n\n\n\nSo, with our z, we get:\n\\(p(x) = \\int p(x|z) p(z|x) dz\\)\nWhere x is the data we want to model, and z is some way of encoding x.\nNow, if we have some model with parameters θ that can generate \\(x\\)s from \\(z\\)s (\\(p_θ(x|z)\\)) then you’d think we could cleverly invert that model somehow such that we can also convert \\(z\\)’s to \\(x\\)’s (\\(p_θ(z|x)\\)).\nWell, that’s theoretically true, but neural networks are too complex for that inversion to be feasible, so we need a separate, additional model called “the encoder” that approximates \\(p(z|x)\\), we’ll call it \\(q_φ(z|x)\\).\n\n\n\nNow, we don’t actually do away with \\(p(z|x)\\) just yet, but using some foresight, we rewrite \\(p(x)\\) as:\n\\[\np(x) = \\int p(x|z) p(z|x) \\frac{q(z|x)}{q(z|x)}dz  \n\\] \\[\n\\ln p(x) = \\ln \\int p(x|z) p(z|x) \\frac{q(z|x)}{q(z|x)}dz\n\\]\nThen… you should just read Paul Rubenstein’s blog post it’s shorter than this post. He goes over the rest of the math to get to ELBO, but they look understandable to me.\nI could re-write them here, but then I’d be pretending to have done mental work I haven’t. I’d just by copy-pasting.\nIf you don’t want to go there, then the main point is that you can separate \\(p(z|x)\\) out of the rest of the above formula into a part that guaranteed to be positive or 0. So that’s how the ELBO is a lower bound and not the real thing.\nThe main thing I would want to know about is Jensen’s Inequality, which I might add in a future git commit."
  },
  {
    "objectID": "elbo.html#finding-the-distribution-of-a-data-set",
    "href": "elbo.html#finding-the-distribution-of-a-data-set",
    "title": "ELBO",
    "section": "",
    "text": "So, back to the blog post.\nWe want to find \\(p(x)\\), the distribution that models all points in a dataset \\(x \\in X\\) and for our generative purposes, we assume that there exists some encoding \\(z\\) that gives our generator information on the \\(x\\) that ought be generated.\n\n\nI’m uncertain, but I think \\(z\\) is necessary due to the discreteness of our computation.\nWe want to model \\(p(x)\\), not just any x. \\(p(x)\\) is a distribution with a lot of information, you don’t get a model to output that entire distribution, you only get a model that can sample from that distribution.\nSo, we are building a model that (approximately) samples \\(p(x)\\). This way, we need some index that informs our model which \\(x\\) we want from \\(p(x)\\).\nAnd what would be extra nice, is if that index already encodes some information about the likelihood of that individual \\(x\\).\nI mean to say, that the instruction you give to the generative model is of the form:\n\nplease generate me an x, that is rare with regards to property \\(z_1, z_2\\) and \\(z_3\\), but has a very typical \\(z_4\\).\n\nSo before it starts generating, we already know how typical the generated \\(x\\) will be, or how likely we are to find points in the dataset that are similar to \\(x\\).\nThis is done by having \\(z\\) be from a known distribution, always usually by sampling \\(z \\sim \\mathcal{N}(0; \\textbf{I})\\). So, you know when the absolute \\(z_i\\) is large, the generated \\(x\\) will be unlikely with respect to the ith element.\nFinally, let it be known that \\(z\\) is not some interpretable variable. \\(z\\) contains information about \\(x\\), but only the model knows how to use that.\n\n\n\nSo, with our z, we get:\n\\(p(x) = \\int p(x|z) p(z|x) dz\\)\nWhere x is the data we want to model, and z is some encoding of x.\nNow, if we have some model with parameters θ that can generate \\(x\\)s from \\(z\\)s (\\(p_θ(x|z)\\)) then you’d think we could cleverly invert that model somehow such that we can also convert \\(z\\)’s to \\(x\\)’s (\\(p_θ(z|x)\\)).\nWell, that’s theoretically true, but neural networks are too complex for that inversion to be feasible, so we need a separate, additional model called “the encoder” that approximates \\(p(z|x)\\), we’ll call it \\(q_φ(z|x)\\).\n\n\n\nNow, we don’t actually do away with \\(p(z|x)\\) just yet, but using some foresight, we rewrite \\(p(x)\\) as:\n\\[\np(x) = \\int p(x|z) p(z|x) \\frac{q(z|x)}{q(z|x)}dz  \n\\] \\[\n\\ln p(x) = \\ln \\int p(x|z) p(z|x) \\frac{q(z|x)}{q(z|x)}dz\n\\]\nThen… you should just read Paul Rubenstein’s blog post it’s shorter than this. He goes over the rest of the math to get to ELBO, but that looks understandable to me.\nI could re-write them here, but then I’d be pretending to have done mental work I haven’t. I’d just by copy-pasting.\nIf you don’t want to go there, then the main point is that you can separate \\(p(z|x)\\) out of the rest of the above formula into a part that guaranteed to be positive or 0.\nYou ignore that part, and optimize for the rest, which is the ELBO.\nThe main thing I still want to understand is Jensen’s Inequality, which I might add here later."
  }
]
---
sidebar: false
page-layout: full
lightbox:
    match: auto
    effect: none
---

# Tech Stack Q&A

<style>
  .sticky-chunk {
    overflow-x: unset !important;
    position: sticky;
    top: 0;
}
</style>
<script src="../res/dark-images.js"></script>

:::: {.columns}

::: {.column width=55%}

> what am I looking at?

It's called "feature-conditional generation" or something like it, the github is called "features2image_diffusion".
It comes down to using a traditional conditional diffusion model with some other network's activations instead of text embeddings of input text.

> for what purpose?

Interpreting a network's activations.
The diffusion model generates data points that look like the original dataset.
If you condition it on the activations, you can see what kind of data the model generates if some activation is exaggerated.

> uhh

See the diagram next to this text.

> ...

So, we have a CNN on the right which is our target for understanding.

> "target for understandig"?

That which we want to understand.

> go on

There are various facets we can try to comprehend about the CNN. I chose to further understanding about the activations of the model.

> why the activations?

There isn't really a correct answer to this. At the end, we need to understand everything: the weights, the architecture, the role of the nonlinearities and the semantics of the activations.
I chose the activations since, intuitively, those seem to contain information we might recognise better.

> Why would we recognise information in activations better than in other parts like the weights?

The activations depend on the specific data point that we feed into the CNN. This specific data point is recognisable to us, so that is a nice anchor point on which we can build further understandings.

Weights on the other hand, are constructed during the entire training run and therefore depend on every data point in the set. For us to understand those, we need something different to anchor on, one might anchor on algorithms we  would expect networks to implement for example. Which anchor to pick there is far less clear however and since nobody seems to have yet succeeded in making broad and accurate guesses about what kind of algorithms networks implement, we currently don't know what kind of anchors are useful for building our understanding upon there.

> With "anchor", do you mean some concrete thing we can reference back to when explaining some part of the CNN?

Yes, I explain the activations of the CNN by showing permutations of the original data point that caused those activations. So the data point is the anchor.  
I also explain the activations in terms of the prediction the CNN makes, that's a second anchor.

> Alright, so you were explaining the diagram that showcases what you did.

So, on the right we have the CNN that we want to understand. In the diagram, I often also show the activations that flow between layers, any box in dotted lines are activations, any undotted box are layers. Lines between those signify activation flow, sometimes with a half-arrow to show the direction.

You can see that I copy the activations before the last layer of the CNN (since two lines flow out of it) and one of the copies goes into the sparse autoencoder (SAE).

> What is that SAE doing there?

Anthropic claims that the elements of activations can be made less polysemantic when you encode them with a SAE.

> What is polysemanticity and why would we care?

Polysemanticity is the property of an activation to encode multiple meanings in the same place.  
Neural networks need to deal with a lot of features, that is, properties of data points that are relevant to keep track of, for the network's task.  
But, only a small subset of all features in the data set, are present in any single data point.
Having a neuron per feature would be inefficient and the network is better off with having the same neurons do different things depending on the context.

A famous example is the fact that one later layer in the GoogLeNet vision model has a neuron that fires on cat faces and car doors. So that is 2 features in a single neuron. The likely explanation is that the rest of the network has little trouble differentiating cats from cars, so this neuron is allowed to be confused between the two.  

To elaborate on this, when the rest of the network is uncertain about whether the image is a cat or a dog, this neuron helps by firing when there is a cat face. When the network is uncertain about whether there is a house or a car, it fires on car doors.  
The instance where the network is uncertain about whether the image is a cat or a car is so rare (since earlier neurons successfully differentiate the two), that this particular neuron does not have to know the difference. It would only be wasteful to have a separate neuron per feature in that layer.

> Okay, so you convert the activation into its features.

Yes, I should mention that this is an optional step though.

> How so?

Well, the success of the SAE is not thoroughly tested. Plausibly it decomposes activations into their features, but there is no guarantee that those features are then more interpretable.

We can also keep the activations as-is. I'll keep calling these copied activations that may or may not have been encoded "features" though.

> Okay, so what are you then doing with these (maybe-encoded) features? You seem to feed them into the "UNET" at some halfway point.

Yes, the U-NET is the network used for diffusion, where diffusion is that process famous for generating images.

When training a diffusion model, we sample an image from a data set, add noise to it and feed it into the U-NET. The task of the U-NET then, is to predict what part of its input is noise and what part is image.  
We call it "noise prediction" since the U-NET needs to output the noise.

> How does noise prediction lead to image generation?

Now, what we can do with a trained U-NET, is to pass complete noise into it, and it then predicts which part of that was noise. It will not output the exact same as the input, since during its training it is used to receiving input that is part image, part noise. So it outputs noise that is similar, but not completely identical to the input.

We _subtract_ this noise then from the input and feed that into the U-NET again. This process repeats for 400 iterations.

> So, at the 400th iteration, you have subtracted all the noise?

Exactly, and what remains is an image that looks like an image from the dataset.

> Okay, so now you've generated an image from say, the MNIST data set. Where do your features come into play in this?

What we have talked about thus far is normal diffusion: generate images from a dataset.  
Normal diffusion lacks direction into which kind of images from the dataset it generates.
Maybe it generates a 1, maybe a 9, we have no control.

> So you are going to use the features to steer the diffusion's generation?

Yes, this is called _conditional diffusion_. This is done by inserting auxiliary information about the image you want to generate, halfway in the U-NET. 

The most-used applications of diffusion (Stable Diffusion, DALL-E, Midjourney) do conditional diffusion with text. Note that you don't just generate an image with those applications, you give it some description that then steers the generation.

During training, they take an image and its caption, they tokenize and embed the caption into some array and then multiply this array with the activations of the U-NET at the midway point in the U-NET, while the U-NET is looking at some noisy version of the image.

The U-NET uses this new information, since this information tells the U-NET what kind of image it should generate: not just any image from the training set, but the specific image that would have this caption.

> Okay, so instead of feeding it text embeddings, you feed it the CNN's features?

Yes. The image that the CNN got, is copied and made noisy, so the U-NET is looking at a noisy version of the same image. Then, at the midpoint, the U-NET receives the activations from the CNN, so it can further steer its generation based on the information the CNN's features provide.

> Ooo-kay, so now you have diffusion model that can generate images based on the CNN's features.

We feed the CNN some image and intercept its features. Then we have the diffusion model generate an image while conditioning it on those features. The diffusion model then generates an image that is very close to the image that the CNN got. So, this part works.

> How does this help with interpreting the CNN?

We have now a U-NET that is trained to interpret the CNN's features.

> Okay, so what?

So what we can do now, is edit those features and see what kind of image the diffusion model generates.

Then, by doing various edits on various parts of the features, we might learn what purposes individual features suit.

> Have you got any results?

Yes, though it is a work in progress. Right now I'm still training various U-NETs on the features of different architectures and sampling from them. Soon, I'll upload more of my results.

:::

::: {.column width=5% .sticky-chunk}

:::

::: {.column width=40% .sticky-chunk}

## Feature-Conditional Diffusion


![](../res/tech-stack.light.png){fig-align="right"}

:::

::::

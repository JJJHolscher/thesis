# ELBO

This project works with diffusion models, which seem to sort-of do the same thing as variational auto-encoders do.
They model some kind of data distribution.

For _some_ reason, VAEs cannot directly optimize themselves to maximize the P(distribution). (What is P(distribution)?) but optimize for some lower bound.

Why? This ELBO keeps coming up and I will need to write about the theoretical foundations of my thesis at some point, so let's just dig into it.

## how we got here

I'm likely going to use OpenAI's ImageNet models, so I was reading the paper. Not that I need to for using them (probably), but if I'm going to use their models, I should cite their paper. It seems appropriate to read it then, and it might make it easier for me to then alter parts of the model since I will need to do that.

Here it is. https://arxiv.org/abs/2102.09672

Now, [here](https://memex.social/a/mPj6CR2ymansKC3HGXsy) they reference the Variational Lower Bound, which google tells me is another way of saying "Evidence Lower Bound" (ELBo).

## sidestepping to calculus 101

[Paul Rubenstein's blog post](http://paulrubenstein.co.uk/deriving-the-variational-lower-bound/) seems to derive the ELBO nicely for us.
But I got distracted by the very common practice of [optimizing for the log of the function instead of the function itself](https://memex.social/a/OkLP7uieI6yi0ZUJGekI).
I trust it's legal to do, but lets go beyond trust this time.

I wanted to find out whether the derivative of log(x) is proportional to that of x.
I forgot what the derivative of that was, so I derived it myself.

![](../res/derivative-of-the-log.pdf)

I gave up and [peeked](https://www.cuemath.com/calculus/derivative-of-ln-x/).
There I was surprised:

$\lim_{t → 0}(1 + t)^{1/t} = e$?

### the binomial theorem

Some [video](https://youtu.be/3Wb0jPhuRco) (don't watch it yet). Immediately mentions the Binomial Theorem.

Apparently, someone found a general formula for finding the coefficients of $(a + b)^x$ after it is expanded. Here expanding means, you remove the parenthesis and write the entire thing out. [Wikipedia's summary](https://en.wikipedia.org/wiki/Binomial_coefficient) was very helpful here.

Apparently $\binom{n}{k}$ means, "give the $k$th coefficient of the expanded form of $(a + b)^n$ ". Go read the wikipedia article for more.

### deriving Euler's number

[This site](https://artofproblemsolving.com/wiki/index.php/Euler%27s_number) points me to the question I should have asked myself.

The one thing I know about $e$ is that $\frac{d}{dx} e^x = e^x$.

So, if we get back to limits, we can find $e$ by solving $a$ for

$\frac{d}{dx} a^x = \lim_{h → 0} \frac{a^{x+h} - a^x}{h} = a^x$

This turned out to be fairly easy:

![](../res/eulers-number.pdf)

So, yes $\lim_{t → 0}(1 + t)^{1/t} = e$

Hmmm, it turns out I hadn't needed Google. That part with the Binomial Theorem primarily was completely unnecessary.

### so, can we use the logarithm when calculating the derivative?


$$\frac{d}{dx} \ln(f(x)) = \frac{1}{f(x)} · \frac{d}{dx} f(x)$$

This is a factor of $\frac{1}{f(x)}$ away from $\frac{d}{dx} f(x)$, which seems fine!  
Just remember to multiply by $f(x)$ whenever you use $\frac{d}{dx} \ln f(x)$ as a substitute for $\frac{d}{dx} f(x)$.

## finding the distribution of a data set

So, back to the blog post.  
We want to find $p(x)$, the distribution that models all points in a dataset $x \in X$ and for our generative purposes, we assume that there exists some encoding $z$ that gives our generator information on the $x$ that ought be generated.

### why the encoding "z"?

I'm uncertain, but I think $z$ is necessary due to the discreteness of our computation.

We want to model $p(x)$, not just any x. $p(x)$ is a distribution with a lot of information, you don't get a model to output that entire distribution, you only get a model that can sample from that distribution.

So, we are building a model that (approximately) samples $p(x)$. This way, we need some index that informs our model _which_ $x$ we want from $p(x)$.

And what would be extra nice, is if that index already encodes some information about the likelihood of that individual $x$.  
I mean to say, that the instruction you give to the generative model is of the form:

> please generate me an x, that is rare with regards to property $z_1, z_2$ and $z_3$, but has a very typical $z_4$.

So before it starts generating, we already know how typical the generated $x$ will be, or how likely we are to find points in the dataset that are similar to $x$.

This is done by having $z$ be from a known distribution, ~~always~~ usually by sampling $z \sim \mathcal{N}(0; \textbf{I})$. So, you know when the absolute $z_i$ is large, the generated $x$ will be unlikely with respect to the ith element.

Finally, let it be known that $z$ is not some interpretable variable. $z$ contains information about $x$, but only the model knows how to use that.

### getting z from x

So, with our z, we get:

$p(x) = \int p(x|z) p(z|x) dz$

Where x is the data we want to model, and z is some encoding of x.

Now, if we have some model with parameters θ that can generate $x$s from $z$s ($p_θ(x|z)$) then you'd think we could cleverly invert that model somehow such that we can also convert $z$'s to $x$'s ($p_θ(z|x)$).

Well, that's theoretically true, but neural networks are too complex for that inversion to be feasible, so we need a separate, additional model called "the encoder" that approximates $p(z|x)$, we'll call it $q_φ(z|x)$.

### the ELBO is in another blog post

Now, we don't actually do away with $p(z|x)$ just yet, but using some foresight, we rewrite $p(x)$ as:

$$
p(x) = \int p(x|z) p(z|x) \frac{q(z|x)}{q(z|x)}dz  
$$
$$
\ln p(x) = \ln \int p(x|z) p(z|x) \frac{q(z|x)}{q(z|x)}dz
$$

Then... you should just read [Paul Rubenstein's blog post](http://paulrubenstein.co.uk/deriving-the-variational-lower-bound/) it's shorter than this.
He goes over the rest of the math to get to ELBO, but that looks understandable to me.

I could re-write them here, but then I'd be pretending to have done mental work I haven't. I'd just by copy-pasting.

If you don't want to go there, then the main point is that you can separate $p(z|x)$ out of the rest of the above formula into a part that guaranteed to be positive or 0.  
You ignore that part, and optimize for the rest, which is the ELBO.

The main thing I still want to understand is Jensen's Inequality, which I might add here later.

